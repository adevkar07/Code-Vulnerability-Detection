# -*- coding: utf-8 -*-
import re
import pandas as pd
import numpy as np
from numpy import array
from numpy import asarray
from numpy import zeros
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Embedding
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Dropout, LSTM, Bidirectional, LeakyReLU
from keras.optimizers import Adamax
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

def plain_model(word_to_index_dict,embedding_matrix,trainable = False, embedding_length = 100, input_length = 10):
    model = Sequential()
    e = Embedding(len(word_to_index_dict.keys()), embedding_length, weights=[embedding_matrix], input_length=input_length, trainable=trainable)
    model.add(e)
    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))
    # compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def blstm(word_to_index_dict,embedding_matrix,trainable = False, embedding_length = 100, input_length = 10):
    model = Sequential()
    e = Embedding(len(word_to_index_dict.keys()), embedding_length, weights=[embedding_matrix], input_length=input_length, trainable=trainable)
    model.add(e)
    model.add(Bidirectional(LSTM(300), input_shape=(len(word_to_index_dict.keys()), embedding_length)))
    model.add(Dense(300))
    model.add(LeakyReLU())
    model.add(Dropout(0.3))
    model.add(Dense(1, activation='softmax'))
    # Lower learning rate to prevent divergence
    adamax = Adamax(lr=0.002)
    model.compile(adamax, 'binary_crossentropy', metrics=['accuracy'])
    return model

def lstm(word_to_index_dict,embedding_matrix,trainable = False, embedding_length = 100, input_length = 10):
    model = Sequential()
    e = Embedding(len(word_to_index_dict.keys()), embedding_length, weights=[embedding_matrix], input_length=input_length, trainable=trainable)
    model.add(e)
    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def test(X_test, y_test, name, batch_size):
    model.load_weights(name)
    values = model.evaluate(X_test, y_test, batch_size=batch_size)
    print("Accuracy is...", values[1])
    predictions = (model.predict(X_test, batch_size=batch_size)).round()
    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()
    print('False positive rate is...', fp / (fp + tn))
    print('False negative rate is...', fn / (fn + tp))
    recall = tp / (tp + fn)
    print('True positive rate is...', recall)
    precision = tp / (tp + fp)
    print('Precision is...', precision)
    print('F1 score is...', (2 * precision * recall) / (precision + recall))


def clean_gadget(gadget):

    cleaned_gadget = []
    rx_comment = re.compile('\*/\s*$')
    
    for line in gadget:
      if rx_comment.search(line) is None:
         cleaned_gadget.append(line)

    return cleaned_gadget

def parse_file(filename):
    with open(filename, "r", encoding="utf8") as file:
        gadget = []
        gadget_val = 0
        for line in file:
            stripped = line.strip()
            if not stripped:
                continue
            if "-" * 33 in line and gadget: 
                yield clean_gadget(gadget),gadget_val
                gadget = []
            elif stripped.split()[0].isdigit():
                if gadget:
                    # Code line could start with number (somehow)
                    if stripped.isdigit():
                        gadget_val = int(stripped)
                    else:
                        gadget.append(stripped)
            else:
                gadget.append(stripped)
        return gadget

def get_embedding_index(filename):
    # load the whole embedding into memory
    embeddings_index = dict()
    word_index = {}
    f = open(filename)
    for line in f:
      values = line.split()
      word = values[0]
      try:  
        coefs = asarray(values[1:], dtype='float32')
      except:
        continue
      embeddings_index[word] = coefs
    f.close()
    print('Loaded %s word vectors.' % len(embeddings_index))
    return embeddings_index

def get_vectors_df(filename,embedding_index, max_length = 10):
    gadgets = []
    count = 0
    labels = []
    word_to_index_dict = dict()
    embedding_matrix = []
    idx = 0
    encoded_docs = []
    for gadget, val in parse_file(filename):
        count += 1
        labels.append(val)
        encoded_gadgets = []
        for index, document in enumerate(gadget):
            for ch in [')','(','*','>','{','}','[',']','(',')']:
                if ch in document:
                    document = document.replace(ch," ")
            gadget[index] = document.split()
            for word in gadget[index]:
                word = word.lower()
                embedding_vector = embedding_index.get(word)
                if word_to_index_dict.get(word) is None:
                    if embedding_vector is not None:
                        word_to_index_dict[word] = idx
                        embedding_matrix.append(embedding_vector)
                        idx+=1
                        encoded_gadgets.append(word_to_index_dict.get(word))
                elif embedding_vector is not None:
                        encoded_gadgets.append(word_to_index_dict.get(word))

        encoded_docs.append(encoded_gadgets)
    
    padded_matrix = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
    
    return word_to_index_dict,padded_matrix,np.array(embedding_matrix), labels

"""CPP2VEC

CWE 119 dataset
"""

print("CWE 119 Train")
embedding_index = get_embedding_index('cpp2vec.txt')
word_to_index_dict, padded_docs, embedding_matrix, labels = get_vectors_df("cwe119_cgd.txt", embedding_index)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = plain_model(word_to_index_dict, embedding_matrix)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("plain_cpp_model.h5")

model.load_weights('plain_cpp_model.h5')
test(X_test, y_test, 'plain_cpp_model.h5', 20)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = blstm(word_to_index_dict, embedding_matrix)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("blstm_cpp_model.h5")

model.load_weights('blstm_cpp_model.h5')
test(X_test, y_test, 'blstm_cpp_model.h5', 20)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = lstm(word_to_index_dict, embedding_matrix)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("lstm_cpp_model.h5")

model.load_weights('lstm_cpp_model.h5')
test(X_test, y_test, 'lstm_cpp_model.h5', 20)

"""CWE 399 dataset"""

print("CWE 399 Train")
embedding_index = get_embedding_index('cpp2vec.txt')
word_to_index_dict, padded_docs, embedding_matrix, labels = get_vectors_df("Cppdata2.txt", embedding_index)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = plain_model(word_to_index_dict, embedding_matrix)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("plain_cpp_model_399.h5")

model.load_weights('plain_cpp_model_399.h5')
test(X_test, y_test, 'plain_cpp_model_399.h5', 20)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = blstm(word_to_index_dict, embedding_matrix)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("blstm_cpp_model_399.h5")

model.load_weights('blstm_cpp_model_399.h5')
test(X_test, y_test, 'blstm_cpp_model_399.h5', 20)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = lstm(word_to_index_dict, embedding_matrix)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("lstm_cpp_model_399.h5")

model.load_weights('lstm_cpp_model_399.h5')
test(X_test, y_test, 'lstm_cpp_model_399.h5', 20)

"""Java2vec"""

embedding_index = get_embedding_index('java2vec.txt')
word_to_index_dict, padded_docs, embedding_matrix, labels = get_vectors_df("Javadata.txt", embedding_index)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = plain_model(word_to_index_dict, embedding_matrix)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("plain_java_model.h5")

model.load_weights('plain_java_model.h5')
test(X_test, y_test, 'plain_java_model.h5', 20)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = blstm(word_to_index_dict, embedding_matrix)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("blstm_java_model.h5")

model.load_weights('blstm_java_model.h5')
test(X_test, y_test, 'blstm_java_model.h5', 20)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = lstm(word_to_index_dict, embedding_matrix)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("lstm_java_model.h5")

model.load_weights('lstm_java_model.h5')
test(X_test, y_test, 'lstm_java_model.h5', 20)

"""C#"""

embedding_index = get_embedding_index('cs2vec.txt')
word_to_index_dict, padded_docs, embedding_matrix, labels = get_vectors_df("Csdata.txt", embedding_index)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = plain_model(word_to_index_dict, embedding_matrix, trainable=True)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("plain_cs_model.h5")

model.load_weights('plain_cs_model.h5')
test(X_test, y_test, 'plain_cs_model.h5', 20)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = blstm(word_to_index_dict, embedding_matrix, trainable=True)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("blstm_cs_model.h5")

model.load_weights('blstm_cs_model.h5')
test(X_test, y_test, 'blstm_cs_model.h5', 20)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = lstm(word_to_index_dict, embedding_matrix, trainable=True)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("lstm_cs_model.h5")

model.load_weights('lstm_cs_model.h5')
test(X_test, y_test, 'lstm_cs_model.h5', 20)



"""inter language vulnerability detector"""

embedding_index = get_embedding_index('cs2vec.txt')
word_to_index_dict, padded_docs, embedding_matrix, labels = get_vectors_df("combined_code.txt", embedding_index)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = plain_model(word_to_index_dict, embedding_matrix)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("plain_all_model.h5")

model.load_weights('plain_all_model.h5')
test(X_test, y_test, 'plain_all_model.h5', 20)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = blstm(word_to_index_dict, embedding_matrix)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("blstm_all_model.h5")

model.load_weights('blstm_all_model.h5')
test(X_test, y_test, 'blstm_all_model.h5', 20)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels,shuffle=True,test_size=0.2)

model = lstm(word_to_index_dict, embedding_matrix)

model.fit(X_train, y_train, epochs=1, verbose=0)

model.save_weights("lstm_all_model.h5")

model.load_weights('lstm_all_model.h5')
test(X_test, y_test, 'lstm_all_model.h5', 20)

